{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, StratifiedKFold\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carrega base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Com categorização das colunas bmi e glicose\n",
    "stroke_cat = pd.read_csv('stroke_df_cat')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrada para modelagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features\n",
    "\n",
    "X = (stroke_cat.drop(['stroke'], axis=1)).values\n",
    "\n",
    "#target\n",
    "\n",
    "y = (stroke_cat['stroke']).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação dos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_clf = DecisionTreeClassifier()\n",
    "knn_clf = KNeighborsClassifier(n_neighbors = 5)\n",
    "rf_clf = RandomForestClassifier(n_estimators=100)\n",
    "lg_clf = LogisticRegression(random_state=0, max_iter=1000)\n",
    "mlp_clf = MLPClassifier(random_state=1, max_iter=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amostragem Holdout teste dos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X, y, test_size = 0.33,stratify=y)\n",
    "\n",
    "#stratify, a função train_test_split garante que a divisão dos dados em conjuntos de treinamento e teste mantém a mesma distribuição das classes em y nos dois conjuntos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "s=SMOTE()\n",
    "X_reso,y_reso =s.fit_resample(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import ClusterCentroids,RandomUnderSampler\n",
    "\n",
    "rus= RandomUnderSampler()\n",
    "cc = ClusterCentroids()\n",
    "\n",
    "X_resu,y_resu = rus.fit_resample(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinar modelos - com Holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Modelo  Accuracy  Precision    Recall  F1-Score\n",
      "0  DecisionTreeClassifier  0.934455   0.204762  0.207729  0.206235\n",
      "1    KNeighborsClassifier  0.955050   0.272727  0.057971  0.095618\n",
      "2  RandomForestClassifier  0.950099   0.241379  0.101449  0.142857\n",
      "3      LogisticRegression  0.958812   0.444444  0.019324  0.037037\n",
      "4           MLPClassifier  0.959010   0.000000  0.000000  0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "modelos = [ad_clf, knn_clf, rf_clf, lg_clf, mlp_clf]\n",
    "\n",
    "# Dicionário para armazenar métricas\n",
    "metrics = {\"Modelo\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1-Score\": []}\n",
    "\n",
    "for modelo in modelos:\n",
    "    modelo.fit(X_train, y_train)\n",
    "    y_pred = modelo.predict(X_test)\n",
    "    \n",
    "    modelo_name = modelo.__class__.__name__\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    metrics[\"Modelo\"].append(modelo_name)\n",
    "    metrics[\"Accuracy\"].append(accuracy)\n",
    "    metrics[\"Precision\"].append(precision)\n",
    "    metrics[\"Recall\"].append(recall)\n",
    "    metrics[\"F1-Score\"].append(f1)\n",
    "\n",
    "# Exiba as métricas\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinar modelos - com Holdout stratify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Modelo  Accuracy  Precision    Recall  F1-Score\n",
      "0  DecisionTreeClassifier  0.930495   0.185841  0.200957  0.193103\n",
      "1    KNeighborsClassifier  0.956634   0.395833  0.090909  0.147860\n",
      "2  RandomForestClassifier  0.948515   0.206897  0.086124  0.121622\n",
      "3      LogisticRegression  0.958020   0.333333  0.014354  0.027523\n",
      "4           MLPClassifier  0.957624   0.272727  0.014354  0.027273\n"
     ]
    }
   ],
   "source": [
    "modelos = [ad_clf, knn_clf, rf_clf, lg_clf, mlp_clf]\n",
    "\n",
    "# Dicionário para armazenar métricas\n",
    "metrics = {\"Modelo\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1-Score\": []}\n",
    "\n",
    "for modelo in modelos:\n",
    "    modelo.fit(X_train_s, y_train_s)\n",
    "    y_pred = modelo.predict(X_test_s)\n",
    "    \n",
    "    modelo_name = modelo.__class__.__name__\n",
    "    accuracy = accuracy_score(y_test_s, y_pred)\n",
    "    precision = precision_score(y_test_s, y_pred)\n",
    "    recall = recall_score(y_test_s, y_pred)\n",
    "    f1 = f1_score(y_test_s, y_pred)\n",
    "    \n",
    "    metrics[\"Modelo\"].append(modelo_name)\n",
    "    metrics[\"Accuracy\"].append(accuracy)\n",
    "    metrics[\"Precision\"].append(precision)\n",
    "    metrics[\"Recall\"].append(recall)\n",
    "    metrics[\"F1-Score\"].append(f1)\n",
    "\n",
    "# Exiba as métricas\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinar modelos com  o KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Modelo  Accuracy  Precision    Recall  F1-Score\n",
      "0  DecisionTreeClassifier  0.931321   0.176610  0.177078  0.164534\n",
      "1    KNeighborsClassifier  0.955760   0.306128  0.060067  0.100375\n",
      "2  RandomForestClassifier  0.949356   0.213365  0.091676  0.115012\n",
      "3      LogisticRegression  0.958309   0.100000  0.003175  0.006154\n",
      "4           MLPClassifier  0.958636   0.000000  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "modelos = [ad_clf, knn_clf, rf_clf, lg_clf, mlp_clf]\n",
    "\n",
    "\n",
    "metrics = {\"Modelo\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1-Score\": []}\n",
    "\n",
    "# Realize a validação cruzada e calcule as métricas para cada modelo\n",
    "for modelo in modelos:\n",
    "    \n",
    "    modelo_name = modelo.__class__.__name__    \n",
    "   \n",
    "    y_pred = cross_val_predict(modelo, X, y, cv=StratifiedKFold(n_splits=5))\n",
    "    \n",
    "    # Calcule as métricas\n",
    "    \n",
    "    accuracy = cross_val_score(modelo, X, y, cv=StratifiedKFold(n_splits=5), scoring='accuracy').mean()\n",
    "    precision = cross_val_score(modelo, X, y, cv=StratifiedKFold(n_splits=5), scoring='precision').mean()\n",
    "    recall = cross_val_score(modelo, X, y, cv=StratifiedKFold(n_splits=5), scoring='recall').mean()\n",
    "    f1 = cross_val_score(modelo, X, y, cv=StratifiedKFold(n_splits=5), scoring='f1').mean()\n",
    "    \n",
    "    # Armazene as métricas no dicionário\n",
    "    metrics[\"Modelo\"].append(modelo_name)\n",
    "    metrics[\"Accuracy\"].append(accuracy)\n",
    "    metrics[\"Precision\"].append(precision)\n",
    "    metrics[\"Recall\"].append(recall)\n",
    "    metrics[\"F1-Score\"].append(f1)\n",
    "\n",
    "# Exiba as métricas\n",
    "import pandas as pd\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinar modelos com  Holdout e undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Modelo  Accuracy  Precision    Recall  F1-Score\n",
      "0  DecisionTreeClassifier  0.721386   0.098930  0.714976  0.173811\n",
      "1    KNeighborsClassifier  0.762970   0.126697  0.811594  0.219178\n",
      "2  RandomForestClassifier  0.753663   0.126710  0.850242  0.220551\n",
      "3      LogisticRegression  0.757624   0.127473  0.840580  0.221374\n",
      "4           MLPClassifier  0.744752   0.120617  0.830918  0.210655\n"
     ]
    }
   ],
   "source": [
    "modelos = [ad_clf, knn_clf, rf_clf, lg_clf, mlp_clf]\n",
    "\n",
    "# Dicionário para armazenar métricas\n",
    "metrics = {\"Modelo\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1-Score\": []}\n",
    "\n",
    "for modelo in modelos:\n",
    "    modelo.fit(X_resu, y_resu)\n",
    "    y_pred = modelo.predict(X_test)\n",
    "    \n",
    "    modelo_name = modelo.__class__.__name__\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    metrics[\"Modelo\"].append(modelo_name)\n",
    "    metrics[\"Accuracy\"].append(accuracy)\n",
    "    metrics[\"Precision\"].append(precision)\n",
    "    metrics[\"Recall\"].append(recall)\n",
    "    metrics[\"F1-Score\"].append(f1)\n",
    "\n",
    "# Exiba as métricas\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinar modelos com  Holdout e oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Modelo  Accuracy  Precision    Recall  F1-Score\n",
      "0  DecisionTreeClassifier  0.875248   0.122995  0.333333  0.179688\n",
      "1    KNeighborsClassifier  0.885545   0.150659  0.386473  0.216802\n",
      "2  RandomForestClassifier  0.895644   0.149123  0.328502  0.205128\n",
      "3      LogisticRegression  0.793465   0.130742  0.714976  0.221060\n",
      "4           MLPClassifier  0.821782   0.130203  0.589372  0.213287\n"
     ]
    }
   ],
   "source": [
    "modelos = [ad_clf, knn_clf, rf_clf, lg_clf, mlp_clf]\n",
    "\n",
    "# Dicionário para armazenar métricas\n",
    "metrics = {\"Modelo\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1-Score\": []}\n",
    "\n",
    "for modelo in modelos:\n",
    "    modelo.fit(X_reso, y_reso)\n",
    "    y_pred = modelo.predict(X_test)\n",
    "    \n",
    "    modelo_name = modelo.__class__.__name__\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    metrics[\"Modelo\"].append(modelo_name)\n",
    "    metrics[\"Accuracy\"].append(accuracy)\n",
    "    metrics[\"Precision\"].append(precision)\n",
    "    metrics[\"Recall\"].append(recall)\n",
    "    metrics[\"F1-Score\"].append(f1)\n",
    "\n",
    "# Exiba as métricas\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinar modelos com  Kfold e undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Modelo  Accuracy  Precision    Recall  F1-Score\n",
      "0  DecisionTreeClassifier  0.740443   0.105679  0.707237  0.183873\n",
      "1    KNeighborsClassifier  0.777757   0.126019  0.737383  0.215205\n",
      "2  RandomForestClassifier  0.760961   0.127641  0.819610  0.220835\n",
      "3      LogisticRegression  0.774946   0.133659  0.810086  0.229439\n",
      "4           MLPClassifier  0.692285   0.108763  0.876653  0.193134\n"
     ]
    }
   ],
   "source": [
    "modelos = [ad_clf, knn_clf, rf_clf, lg_clf, mlp_clf]\n",
    "metrics = {\"Modelo\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1-Score\": []}\n",
    "\n",
    "n_splits = 5\n",
    "cv = StratifiedKFold(n_splits=n_splits)\n",
    "under = RandomUnderSampler()\n",
    "\n",
    "\n",
    "for modelo in modelos:\n",
    "\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []    \n",
    "\n",
    "    # Realize a validação cruzada com undersampling\n",
    "    for train_idx, test_idx in cv.split(X, y):\n",
    "        X_train, y_train = X[train_idx], y[train_idx]\n",
    "        X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "    # Realize o undersampling nos dados de treinamento\n",
    "        X_resampled, y_resampled =under.fit_resample(X_train, y_train)\n",
    "    \n",
    "        modelo_name = modelo.__class__.__name__  \n",
    "        modelo.fit(X_resampled, y_resampled)\n",
    "        y_pred = modelo.predict(X_test)\n",
    "    \n",
    "        modelo_name = modelo.__class__.__name__\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "        \n",
    "    mean_accuracy = sum(accuracies) / n_splits\n",
    "    mean_precision = sum(precisions) / n_splits\n",
    "    mean_recall = sum(recalls) / n_splits\n",
    "    mean_f1 = sum(f1s) / n_splits\n",
    "\n",
    "    # Armazene as métricas no dicionário\n",
    "    metrics[\"Modelo\"].append(modelo_name)\n",
    "    metrics[\"Accuracy\"].append(mean_accuracy)    \n",
    "    metrics[\"Precision\"].append(mean_precision)\n",
    "    metrics[\"Recall\"].append(mean_recall)\n",
    "    metrics[\"F1-Score\"].append(mean_f1)\n",
    "\n",
    "# Exiba as métricas\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "print(metrics_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinar modelos com  Kfold e oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Modelo  Accuracy  Precision    Recall  F1-Score\n",
      "0  DecisionTreeClassifier  0.877409   0.122715  0.319648  0.177194\n",
      "1    KNeighborsClassifier  0.889237   0.171308  0.436733  0.245996\n",
      "2  RandomForestClassifier  0.888714   0.136014  0.316448  0.190114\n",
      "3      LogisticRegression  0.799908   0.133637  0.701037  0.224464\n",
      "4           MLPClassifier  0.822387   0.135607  0.606049  0.221138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "modelos = [ad_clf, knn_clf, rf_clf, lg_clf, mlp_clf]\n",
    "metrics = {\"Modelo\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1-Score\": []}\n",
    "\n",
    "n_splits = 5\n",
    "cv = StratifiedKFold(n_splits=n_splits)\n",
    "over = SMOTE()\n",
    "\n",
    "\n",
    "for modelo in modelos:\n",
    "\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []    \n",
    "\n",
    "    # Realize a validação cruzada com undersampling\n",
    "    for train_idx, test_idx in cv.split(X, y):\n",
    "        X_train, y_train = X[train_idx], y[train_idx]\n",
    "        X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "    # Realize o undersampling nos dados de treinamento\n",
    "        X_resampled, y_resampled =over.fit_resample(X_train, y_train)\n",
    "    \n",
    "        modelo_name = modelo.__class__.__name__  \n",
    "        modelo.fit(X_resampled, y_resampled)\n",
    "        y_pred = modelo.predict(X_test)\n",
    "    \n",
    "        modelo_name = modelo.__class__.__name__\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "        \n",
    "    mean_accuracy = sum(accuracies) / n_splits\n",
    "    mean_precision = sum(precisions) / n_splits\n",
    "    mean_recall = sum(recalls) / n_splits\n",
    "    mean_f1 = sum(f1s) / n_splits\n",
    "\n",
    "    # Armazene as métricas no dicionário\n",
    "    metrics[\"Modelo\"].append(modelo_name)\n",
    "    metrics[\"Accuracy\"].append(mean_accuracy)    \n",
    "    metrics[\"Precision\"].append(mean_precision)\n",
    "    metrics[\"Recall\"].append(mean_recall)\n",
    "    metrics[\"F1-Score\"].append(mean_f1)\n",
    "\n",
    "# Exiba as métricas\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {\n",
    "    'Decision Tree': Pipeline([\n",
    "        ('classifier', DecisionTreeClassifier())\n",
    "    ]),\n",
    "    'Random Forest': Pipeline([\n",
    "        ('classifier', RandomForestClassifier())\n",
    "    ]),\n",
    "    'MLP Classifier': Pipeline([\n",
    "        ('classifier', MLPClassifier())\n",
    "    ]),\n",
    "    'KNN': Pipeline([\n",
    "        ('classifier', KNeighborsClassifier())\n",
    "    ]),\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    'Random Forest':{\n",
    "        'classifier__n_estimators': [50,100,200],\n",
    "        'classifier__max_depth': [2,4,6]\n",
    "    },\n",
    "    'Decision Tree':{\n",
    "        'classifier__min_samples_split': [2,3,4],\n",
    "        'classifier__max_depth': [2,4,6]\n",
    "    },\n",
    "    'KNN':{\n",
    "        'classifier__n_neighbors': [3,5,7],\n",
    "        'classifier__weights': ['uniform', 'distance']\n",
    "    },\n",
    "    'MLP Classifier':{\n",
    "        'classifier__hidden_layer_sizes': [(50,), (100,50), (100,100,50)],\n",
    "        'classifier__alpha': [0.0001, 0.001, 0.01]\n",
    "    }\n",
    "}\n",
    "\n",
    "for model_name, pipeline in pipelines.items():\n",
    "  grid_search = GridSearchCV(pipeline, param_grids[model_name], scoring='accuracy')\n",
    "  grid_search.fit(X_train, y_train)\n",
    "\n",
    "  best_params = grid_search.best_params_\n",
    "  best_model = grid_search.best_estimator_\n",
    "\n",
    "  y_pred = best_model.predict(X_test)\n",
    "  accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "  print(\"Modelo:\", model_name)\n",
    "  print(\"Melhores Parâmetros:\", best_params)\n",
    "  print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
